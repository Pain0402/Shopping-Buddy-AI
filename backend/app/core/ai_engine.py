from PIL import Image
from transformers import CLIPProcessor, CLIPModel
import torch
import io

class AIEngine:
    _instance = None

    def __new__(cls):
        # Singleton Pattern: Ch·ªâ t·∫°o instance n·∫øu ch∆∞a c√≥
        if cls._instance is None:
            cls._instance = super(AIEngine, cls).__new__(cls)
            cls._instance.initialize()
        return cls._instance

    def initialize(self):
        print("üöÄ ƒêang t·∫£i CLIP Model... (Vi·ªác n√†y s·∫Ω t·ªën ch√∫t th·ªùi gian l·∫ßn ƒë·∫ßu)")
        # S·ª≠ d·ª•ng model patch32 (nh·∫π h∆°n, nhanh h∆°n, ƒë·ªô ch√≠nh x√°c ·ªïn)
        model_id = "openai/clip-vit-base-patch32"
        
        self.model = CLIPModel.from_pretrained(model_id)
        self.processor = CLIPProcessor.from_pretrained(model_id)
        print("‚úÖ CLIP Model ƒë√£ s·∫µn s√†ng!")

    def create_embedding(self, image_bytes: bytes):
        """
        Input: ·∫¢nh d·∫°ng bytes
        Output: Vector 512 chi·ªÅu (List[float])
        """
        # 1. Chuy·ªÉn bytes th√†nh PIL Image
        image = Image.open(io.BytesIO(image_bytes))
        
        # 2. Ti·ªÅn x·ª≠ l√Ω (Resize, Normalize theo chu·∫©n OpenAI)
        inputs = self.processor(images=image, return_tensors="pt")
        
        # 3. Ch·∫°y Inference (Kh√¥ng t√≠nh gradient ƒë·ªÉ ti·∫øt ki·ªám RAM)
        with torch.no_grad():
            image_features = self.model.get_image_features(**inputs)
        
        # 4. Chu·∫©n h√≥a vector (Normalization) ƒë·ªÉ d√πng Cosine Similarity
        image_features /= image_features.norm(dim=-1, keepdim=True)
        
        # 5. Chuy·ªÉn Tensor th√†nh List Python th∆∞·ªùng
        return image_features.squeeze().tolist()

# T·∫°o bi·∫øn to√†n c·ª•c ƒë·ªÉ c√°c file kh√°c import d√πng lu√¥n
ai_engine = AIEngine()